{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network\n",
    "\n",
    "This notebook serves as example to show you how you can train and test a model yourself based on your own data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "# add dicrectories of this repo to system path for and then import necessary functions\n",
    "sys.path.append(os.path.dirname(os.getcwd())+'/src') \n",
    "from utils import *\n",
    "from model import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and Configurations\n",
    "\n",
    "First you need to to configure which features to use, plus the width and overlap of the sliding windows for feature creation. The configurations refer to the data examples provded in the ```data/customer_data/``` folder. The assumption is that the data of every individual household is stored in one file and that the energy consumption and weather data are along with their reference to the timestamp are stored in the same data frame. While we use energy data in 15 min resolution, you could potentially also train models on other resolutions. However, you then may want to make adjustments in the feature calculation (especially with respect to the temporal features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "# FEATURES CONFIGURATION\n",
    "#--------------------------\n",
    "feat_time = True # whether or not to encode the time as cyclic features \n",
    "feat_energymean = True # whether or not to use the mean energy of the sequence as feature\n",
    "feat_weather = True # whether or not to use the weather as feature \n",
    "feat_dhw = True # whether or not to use the DHW production as feature\n",
    "weather_columns = ['daily_avgtemp', 'hourly_avgtemp', 'daily_maxtemp', 'daily_mintemp'] # columns in data frame to be used for weather features\n",
    "\n",
    "#-----------------------------\n",
    "# SLIDING WINDOW CONFIGURATION\n",
    "#-----------------------------\n",
    "window_length = 8 # width of sliding window - 8 values for 15 min intervals means 2 hours\n",
    "window_overlap = 4 # overleap of sliding windows - 4 values for 15 min intervals means 1 hour\n",
    "\n",
    "#------------------------------------------\n",
    "# DEFINING COLUMNS TO BE USED OF DATA FRAME\n",
    "#------------------------------------------\n",
    "key_consumption_input = 'kWh_aggregated' # column name of the input energy consumption\n",
    "key_consumption_output = 'kWh_heat_pump' # column name of the output energy consumption to be learned\n",
    "key_timestamp = 'timestamp' # column name of the timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sliding Window Data\n",
    "The following code prepares the arrays of features used as input by the neural network and the arrays of targets (energy sequences) to be learned by the network. The corresponding functions to prepare these arrays assume that the data is stored in one data frame per customer, which holds the energy consumption and weather data as separate columns. Consequently, the example below creates a dictionary to hold the pre-processed data for each customer. The keys of the dictionary are the customerIDs and the the values are dictionaries with the input and output value for the network in form of arrays. In these arrays each row corresponds to one sliding window calculated from the original time series. This approach provides more flexibility to decide in a later step which households to use for training and which for testing.\n",
    "\n",
    "**NOTE:** The code below depends on the settings chosen above (e.g. in terms of which features to be used and which sliding window configurations). Also the order of features matters, as the models provided in this repository were trained with the features in the order defined here. The code below however also shows you how you can save and restore the data dictionary created, so that for large data sets you only have to create it once per configuration. Of course, in general, the approach below only serves as example and you are open to handle the feature creation in any other way that fits to you workflow. \n",
    "\n",
    "**Note (once again): if you change the feature-related settings above, you need to rerun the followign cell to create a new data dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data dictionary to file...\n",
      "Data dictionary saved to file.\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------\n",
    "# OPTIONAL: LOAD DICTIONARY IF IT HAS BEEN SAVED EARLIER\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# define the folder where the dictionary should be saved and in case it does not exist create it\n",
    "folder = os.path.dirname(os.getcwd()) + '/data_temp/'\n",
    "mkdir(folder)\n",
    "\n",
    "# create a unique file name according to the features and sliding window configuration\n",
    "file_name = 'data_dict_time{}_energymean{}_weather{}_dhw{}_windowlength{}_overlap{}_weathercols{}.pkl'.format(feat_time, feat_energymean, feat_weather, feat_dhw, window_length, window_overlap, str(weather_columns).replace('\\'', '').replace(' ', '').replace('[', '').replace(']', '').replace(',', '-'))\n",
    "\n",
    "# when the dictionary has been created previously, load it from file\n",
    "if file_exists(folder + file_name):\n",
    "\n",
    "    print('Data dictionary already exists. Loading it from file...')\n",
    "    with open(folder + file_name, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    print('Data dictionary loaded from file.')\n",
    "\n",
    "#--------------------------\n",
    "# PREPARE DATA DICTIONARY\n",
    "#--------------------------\n",
    "\n",
    "else: \n",
    "    print('Creating data dictionary...')\n",
    "\n",
    "    # load overview of systems with available data \n",
    "    df_systems = pd.read_csv(os.path.dirname(os.getcwd()) + '/data/meta_data.csv')\n",
    "\n",
    "    # create a dictionary to save the data with customerID as key and a dictionary with features and targets as value\n",
    "    data_dict = {}\n",
    "\n",
    "    # loop over each HP system and create features and targets \n",
    "    for idx, row in tqdm(df_systems.iterrows(), total=df_systems.shape[0]):\n",
    "\n",
    "        # get customer id and information whether HP is responsible for DHW production\n",
    "        customer_id = row['customer_id']\n",
    "        dhw_value = row['dhw_production']\n",
    "\n",
    "        # load original smart meter and weather data frame \n",
    "        df = pd.read_csv('{}/data/customer_data/{}.csv'.format(os.path.dirname(os.getcwd()), customer_id))\n",
    "\n",
    "        # check that the data is valid \n",
    "        if len(df) == 0: \n",
    "            continue\n",
    "\n",
    "        # create the features as input to the network (i.e. X)\n",
    "        np_features = create_features_array_from_original_data_frame(df, window_length, window_overlap, key_consumption_input, dhw_value=dhw_value, key_timestamp=key_timestamp, feat_energymean=feat_energymean, feat_time=feat_time, feat_weather=feat_weather, feat_dhw=feat_dhw, weather_columns=weather_columns)\n",
    "        \n",
    "        # create the targets as output of the network (i.e. Y)\n",
    "        np_targets = create_features_array_from_original_data_frame(df, window_length, window_overlap, key_consumption_output, dhw_value=dhw_value, key_timestamp=key_timestamp, feat_energymean=False, feat_time=False, feat_weather=False, feat_dhw=False, weather_columns=weather_columns)\n",
    "\n",
    "        # make sure that features and targets match in their length\n",
    "        if np_targets.shape[0] > np_features.shape[0]:\n",
    "            np_targets = np.delete(np_targets, -1, axis=0)\n",
    "\n",
    "        elif np_targets.shape[0] < np_features.shape[0]:\n",
    "            np_features = np.delete(np_features, -1, axis=0)\n",
    "\n",
    "        np.testing.assert_equal(np_features.shape[0], np_targets.shape[0])\n",
    "\n",
    "        # add to dictionary \n",
    "        data_dict[customer_id] = {'X': np_features, 'y': np_targets, 'original_data' : df}\n",
    "\n",
    "#----------------------------------------------\n",
    "# OPTIONAL: SAVE DATA DICTIONARY FOR LATER USE\n",
    "#----------------------------------------------\n",
    "    \n",
    "# save the dictionary\n",
    "if not file_exists(folder+file_name):\n",
    "\n",
    "    print('Saving data dictionary to file...')\n",
    "    with open(folder + file_name, 'wb') as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "    print('Data dictionary saved to file.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test Set Through Concatenation\n",
    "In the previous cell, we created the features and targets for our desired configuration and stored them in a dictionary (for each household separately). Next, we need to decide which households to consider for training and which for testing. In the file ```data/meta_data.csv```, we list which household belong to which fold of a 5-fold cross validation. Therefore, we just select any fold and use all corresponding households for testing, while all others are used for training. \n",
    "\n",
    "**NOTE:** In practice, you would probably combine this cell with the cells below and loop over the folds to train and evaluate a single model per fold to then calculate the average scores across all folds. At least, this is what we did in the paper. Although not implemented here, you could consider storing and loading the final feature and target arrays for each fold using ```np.save(...)``` and ```np.load(...)```, if your decision which household is part of which fold does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 270.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------\n",
    "# SELECT FOLD\n",
    "#----------------------------------------\n",
    "\n",
    "# NOTE: the current fold is the one used for testing and all other folds are used for training \n",
    "# NOTE: in practice you would loop over the folds and train and evaluate the model for each fold, then average the scores for final reporting \n",
    "current_fold = 0\n",
    "\n",
    "# define households to be used for training and evaluation \n",
    "df_systems = pd.read_csv(os.path.dirname(os.getcwd()) + '/data/meta_data.csv')\n",
    "households_train = df_systems[df_systems['kfold'] != current_fold]['customer_id'].values\n",
    "households_test = df_systems[df_systems['kfold'] == current_fold]['customer_id'].values\n",
    "\n",
    "#----------------------------------------\n",
    "# PREPARE TRAINING DATA SET \n",
    "#----------------------------------------\n",
    "\n",
    "# create training set by concatenating features and targets of the different households that should be used for training\n",
    "X_train = None \n",
    "y_train = None\n",
    "\n",
    "# loop over training households and concatenate\n",
    "for customer_id in tqdm(households_train):\n",
    "\n",
    "    # create features \n",
    "    if X_train is None:\n",
    "        X_train = data_dict[customer_id]['X']\n",
    "    else: \n",
    "        X_train = np.concatenate([X_train, data_dict[customer_id]['X']], axis=0)\n",
    "\n",
    "    # create targets\n",
    "    if y_train is None:\n",
    "        y_train = data_dict[customer_id]['y']\n",
    "    else:\n",
    "        y_train = np.concatenate([y_train, data_dict[customer_id]['y']], axis=0)\n",
    "\n",
    "# drop rows with nan values in either X_train or y_train - but needs to be done in both arrays correspondingly\n",
    "indices = np.where(np.isnan(X_train).any(axis=0))[0].tolist() + np.where(np.isnan(y_train).any(axis=0))[0].tolist()\n",
    "if len(indices) > 0:\n",
    "    print('Dropping {} rows due to nan values in either features or targets.'.format(len(indices)))\n",
    "    X_train = np.delete(X_train, indices, axis=0)\n",
    "    y_train = np.delete(y_train, indices, axis=0)\n",
    "\n",
    "# NOTE: test data set is not created here because evaluation will happen in a later stage and will rather be handled individually per household"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train the Model\n",
    "Below, an exemplary model is trained for 20 epochs. While we keep the configuration simple, note that more parameters can be configured (see the ```__init__()```-function in ```model.py```.) The parameter ```apple_silicon``` should be used for Apple silicon chips as it would choose a legacy version of the Adam optimizer, which is recommended with this version of tensorflow configured during anaconda installation of this repo. \n",
    "\n",
    "**NOTE:** The code is currently written in such a way that it uses all CPU available, but does not consider GPU training or any other types of speed optimization because this is highly individual. You may want to adjust these things according to your own needs and setup after forking the repo. Please consider the code provided just as the bare minimum to start with. \n",
    "\n",
    "**NOTE:** With the ```basepath``` parameter you have the option to provide the model a path to a folder where it should store the models and tensorboard files. When ```basepath=None```, the model instance will create a folder named ```results/<model_name>``` to store relevant files and tensorboards will be stored in ```results/tensorboards/<model_name>```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/02/19 17:04:50 test_model] [DEBUG] Handler: Intialization successful. Name: test_model\n",
      "[24/02/19 17:04:50 test_model] [DEBUG] Model.set_epochs(): Number of epochs set to 20.\n",
      "[24/02/19 17:04:50 test_model] [DEBUG] Model.set_batch_size(): Batch size set to 2048.\n",
      "[24/02/19 17:04:50 test_model] [DEBUG] Model.__init__(): Object creation successful.\n",
      "[24/02/19 17:04:50 test_model] [INFO] Model.fit(): Starting training.\n",
      "[24/02/19 17:04:50 test_model] [DEBUG] Model.create_callbacks(): Callbacks created.\n",
      "[24/02/19 17:04:50 test_model] [DEBUG] Model.__init_nn__(): Initialization successful.\n",
      "Epoch 1/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0656 - root_mean_squared_error: 0.2561 - mean_absolute_error: 0.1276 - val_loss: 0.0261 - val_root_mean_squared_error: 0.1615 - val_mean_absolute_error: 0.0883 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0205 - root_mean_squared_error: 0.1432 - mean_absolute_error: 0.0760 - val_loss: 0.0183 - val_root_mean_squared_error: 0.1352 - val_mean_absolute_error: 0.0693 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0172 - root_mean_squared_error: 0.1310 - mean_absolute_error: 0.0680 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1273 - val_mean_absolute_error: 0.0657 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "234/234 [==============================] - 1s 4ms/step - loss: 0.0163 - root_mean_squared_error: 0.1278 - mean_absolute_error: 0.0659 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1294 - val_mean_absolute_error: 0.0651 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1244 - mean_absolute_error: 0.0635 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1235 - val_mean_absolute_error: 0.0650 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "234/234 [==============================] - 1s 4ms/step - loss: 0.0152 - root_mean_squared_error: 0.1232 - mean_absolute_error: 0.0629 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1252 - val_mean_absolute_error: 0.0617 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0145 - root_mean_squared_error: 0.1205 - mean_absolute_error: 0.0613 - val_loss: 0.0144 - val_root_mean_squared_error: 0.1200 - val_mean_absolute_error: 0.0630 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "234/234 [==============================] - 1s 4ms/step - loss: 0.0142 - root_mean_squared_error: 0.1191 - mean_absolute_error: 0.0606 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1231 - val_mean_absolute_error: 0.0606 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0137 - root_mean_squared_error: 0.1171 - mean_absolute_error: 0.0595 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1174 - val_mean_absolute_error: 0.0607 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0135 - root_mean_squared_error: 0.1163 - mean_absolute_error: 0.0593 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1156 - val_mean_absolute_error: 0.0600 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0131 - root_mean_squared_error: 0.1146 - mean_absolute_error: 0.0583 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1147 - val_mean_absolute_error: 0.0578 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0129 - root_mean_squared_error: 0.1137 - mean_absolute_error: 0.0578 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1140 - val_mean_absolute_error: 0.0570 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0127 - root_mean_squared_error: 0.1127 - mean_absolute_error: 0.0573 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1128 - val_mean_absolute_error: 0.0584 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0124 - root_mean_squared_error: 0.1115 - mean_absolute_error: 0.0566 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1108 - val_mean_absolute_error: 0.0558 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0122 - root_mean_squared_error: 0.1105 - mean_absolute_error: 0.0560 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1107 - val_mean_absolute_error: 0.0561 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "234/234 [==============================] - 1s 4ms/step - loss: 0.0121 - root_mean_squared_error: 0.1098 - mean_absolute_error: 0.0558 - val_loss: 0.0125 - val_root_mean_squared_error: 0.1120 - val_mean_absolute_error: 0.0576 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0119 - root_mean_squared_error: 0.1090 - mean_absolute_error: 0.0553 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1087 - val_mean_absolute_error: 0.0546 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "234/234 [==============================] - 1s 5ms/step - loss: 0.0117 - root_mean_squared_error: 0.1082 - mean_absolute_error: 0.0549 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1068 - val_mean_absolute_error: 0.0538 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "234/234 [==============================] - 1s 4ms/step - loss: 0.0115 - root_mean_squared_error: 0.1070 - mean_absolute_error: 0.0541 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1076 - val_mean_absolute_error: 0.0550 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "234/234 [==============================] - 1s 4ms/step - loss: 0.0115 - root_mean_squared_error: 0.1071 - mean_absolute_error: 0.0543 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1095 - val_mean_absolute_error: 0.0576 - lr: 0.0010\n",
      "[24/02/19 17:05:12 test_model] [DEBUG] Model.fit(): Reloaded best model from checkpoint.\n",
      "[24/02/19 17:05:12 test_model] [INFO] Model.save_x_scaler(): X_Scaler saved as: /Users/tobiasbrudermueller/Documents/Code/smd-hp-disaggregation/results/test_model/scaler/Xscaler.sav\n",
      "[24/02/19 17:05:12 test_model] [INFO] Model.fit(): Finished training.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------\n",
    "# CREATE AND TRAIN MODEL \n",
    "#----------------------------------------\n",
    "        \n",
    "# define model parameters\n",
    "# NOTE: more parameters can be configured - see model.py --> __init__() for more details - for this example, we just keep it simple\n",
    "layer_shapes = [50, 100, 200, 100, 50]\n",
    "epochs = 20\n",
    "model_name = 'test_model' # NOTE: if no model name is provided, the model instance would create a unique model name based on the fold and the chosen configs\n",
    "basepath = None # option to provide a path to a folder where the models and tensorboards should be saved - if None, results folder in this repo will be created\n",
    "apple_silicon = True\n",
    "\n",
    "# create model \n",
    "model = Model(model_name, current_fold, layer_shapes=layer_shapes, epochs=epochs, apple_silicon=apple_silicon, basepath=basepath)\n",
    "\n",
    "# train model \n",
    "model.fit(X_train, y_train, validation_split=0.1, shuffle_data=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model With Test Data\n",
    "\n",
    "To evaluate the model, we loop over all households belonging to the fold that was not used for training. Using the ```ResultReporter``` class from the ```resultreporter.py```-file, we can calculate the score per household. The model evaluation can then be done by averaging across the scores.\n",
    "\n",
    "**NOTE:** In this repo, we only provide a minimum amount of data to show you how to use the code, therefore the results below cannot be interpreted as reflecting real performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>meanAbsoluteError</th>\n",
       "      <th>meanSquaredError</th>\n",
       "      <th>rootMeanSquaredError</th>\n",
       "      <th>rootMeanSquaredLogError</th>\n",
       "      <th>R2Score</th>\n",
       "      <th>maxResidualError</th>\n",
       "      <th>medianAbsoluteError</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8110168</td>\n",
       "      <td>0.395554</td>\n",
       "      <td>0.449508</td>\n",
       "      <td>0.670453</td>\n",
       "      <td>-0.399801</td>\n",
       "      <td>-0.705767</td>\n",
       "      <td>3.733869</td>\n",
       "      <td>0.189453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2104831</td>\n",
       "      <td>0.389896</td>\n",
       "      <td>0.270485</td>\n",
       "      <td>0.520082</td>\n",
       "      <td>-0.653769</td>\n",
       "      <td>-0.127457</td>\n",
       "      <td>3.602547</td>\n",
       "      <td>0.300877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1077878</td>\n",
       "      <td>0.166093</td>\n",
       "      <td>0.072521</td>\n",
       "      <td>0.269297</td>\n",
       "      <td>-1.311939</td>\n",
       "      <td>0.674504</td>\n",
       "      <td>2.180855</td>\n",
       "      <td>0.075477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1058711</td>\n",
       "      <td>1.091673</td>\n",
       "      <td>4.989607</td>\n",
       "      <td>2.233743</td>\n",
       "      <td>0.803679</td>\n",
       "      <td>-40.173324</td>\n",
       "      <td>9.332566</td>\n",
       "      <td>0.465779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1222107</td>\n",
       "      <td>0.386123</td>\n",
       "      <td>0.672930</td>\n",
       "      <td>0.820323</td>\n",
       "      <td>-0.198057</td>\n",
       "      <td>-7.262323</td>\n",
       "      <td>5.046990</td>\n",
       "      <td>0.202847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold customer_id  meanAbsoluteError  meanSquaredError  \\\n",
       "0     0     8110168           0.395554          0.449508   \n",
       "1     0     2104831           0.389896          0.270485   \n",
       "2     0     1077878           0.166093          0.072521   \n",
       "3     0     1058711           1.091673          4.989607   \n",
       "4     0     1222107           0.386123          0.672930   \n",
       "\n",
       "   rootMeanSquaredError  rootMeanSquaredLogError    R2Score  maxResidualError  \\\n",
       "0              0.670453                -0.399801  -0.705767          3.733869   \n",
       "1              0.520082                -0.653769  -0.127457          3.602547   \n",
       "2              0.269297                -1.311939   0.674504          2.180855   \n",
       "3              2.233743                 0.803679 -40.173324          9.332566   \n",
       "4              0.820323                -0.198057  -7.262323          5.046990   \n",
       "\n",
       "   medianAbsoluteError  \n",
       "0             0.189453  \n",
       "1             0.300877  \n",
       "2             0.075477  \n",
       "3             0.465779  \n",
       "4             0.202847  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------------------------\n",
    "# SELECT FOLD\n",
    "#----------------------------------------\n",
    "\n",
    "# NOTE: the current fold is the one used for testing and all other folds are used for training \n",
    "# NOTE: in practice you would loop over the folds and train and evaluate the model for each fold, then average the scores for final reporting \n",
    "current_fold = 0\n",
    "\n",
    "# define households to be used for training and evaluation \n",
    "df_systems = pd.read_csv(os.path.dirname(os.getcwd()) + '/data/meta_data.csv')\n",
    "households_train = df_systems[df_systems['kfold'] != current_fold]['customer_id'].values\n",
    "households_test = df_systems[df_systems['kfold'] == current_fold]['customer_id'].values\n",
    "\n",
    "#--------------------------------------------\n",
    "# APPLY MODEL TO TEST HOUSEHOLDS AND EVALUATE\n",
    "#--------------------------------------------\n",
    "\n",
    "# create a data frame to store the evaluation scores for all households \n",
    "df_eval = None \n",
    "\n",
    "# loop over customer households \n",
    "for customer_id in tqdm(households_test):\n",
    "\n",
    "    # get sliding window features and original data frame from the data dictionary\n",
    "    np_test_features = data_dict[customer_id]['X']\n",
    "    df_original = data_dict[customer_id]['original_data']\n",
    "\n",
    "    # make a prediction for the test features \n",
    "    np_test_predictions = model.predict(np_test_features)\n",
    "\n",
    "    # transform the sliding window predictions back into time series and add as new column to original data frame\n",
    "    df_predicted = create_data_series_from_sliding_windows(np_test_predictions, window_overlap, df_original=df_original, new_column_name='kWh_heat_pump_predicted')\n",
    "    df_predicted.dropna(subset=['kWh_heat_pump_predicted', 'kWh_heat_pump'], inplace=True)\n",
    "\n",
    "    # create an instance of the result reporter, which calculates the scores - NOTE: need to create one instance per household! \n",
    "    reporter = ResultReporter('regression', groundtruth=df_predicted['kWh_heat_pump'].values, predictions=df_predicted['kWh_heat_pump_predicted'].values)\n",
    "    df_scores = reporter.getResultDataFrame(only_relevant_metrics=True, parameter_evaluated='{}'.format(customer_id))\n",
    "    if df_eval is None:\n",
    "        df_eval = df_scores\n",
    "    else: \n",
    "        df_eval = pd.concat([df_eval, df_scores], axis=0)\n",
    "\n",
    "# final adjustments to evaluation scores\n",
    "df_eval.reset_index(inplace=True, drop=True)\n",
    "df_eval.drop(columns=['task_type'], inplace=True)\n",
    "df_eval.rename(columns={'parameter_evaluated': 'customer_id'}, inplace=True)\n",
    "df_eval.insert(0, 'fold', current_fold)\n",
    "\n",
    "# save the evaluation scores to file\n",
    "handler = model.get_handler()\n",
    "df_eval.to_csv(handler.basepath + 'evaluation_scores_fold{}.csv'.format(current_fold), index=False)\n",
    "display(df_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kiwo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
